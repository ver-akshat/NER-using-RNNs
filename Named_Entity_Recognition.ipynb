{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, I am trying am trying to perform Named Entity Recognition with RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "seed=54321\n",
    "% env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data\n",
    "url='https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
    "dir_name='data'\n",
    "def download_data(url,filename,download_dir):\n",
    "    \"\"\"\n",
    "    Download a file if its not present\n",
    "    \"\"\"\n",
    "    # create directory is it not exists\n",
    "    os.makedirs(download_dir,exist_ok=True)\n",
    "    # if file do not exist , download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath,_=urlretrieve(url+filename,os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath=os.path.join(download_dir,filename)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train, test and validation sets.\n",
    "train_filepath = download_data(url, 'conllpp_train.txt', dir_name)\n",
    "dev_filepath = download_data(url, 'conllpp_dev.txt', dir_name)\n",
    "test_filepath = download_data(url, 'conllpp_test.txt', dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data looks like:\n",
    " -DOCSTART- -X- -X- O  \n",
    " \n",
    " EU NNP B-NP B-ORG  \n",
    "\n",
    " rejects VBZ B-VP O  \n",
    "\n",
    " German JJ B-NP B-MISC  \n",
    " \n",
    " call NN I-NP O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "def read_data(filename):\n",
    "    '''\n",
    "    read data from a file with given filename.\n",
    "    Return a list of sentence and list of ner labels\n",
    "    '''\n",
    "    print('Reading data')\n",
    "    sentences,ner_labels=[],[]\n",
    "    # open the file\n",
    "    with open(filename,'r',encoding='latin-1') as f:\n",
    "        # read each line\n",
    "        is_sos=True # record at each sentence if it is start of sentence\n",
    "        sentence_tokens=[]\n",
    "        sentence_labels=[]\n",
    "        i=0\n",
    "        for row in f:\n",
    "            # if there is empty line or 'DOCSTART' its a new line\n",
    "            if len(row.strip())==0 or row.split(' ')[0]=='DOCSTART':\n",
    "                is_sos=False\n",
    "            else:\n",
    "                is_sos=True\n",
    "                token,_,_,ner_label=row.split()\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "            # when end of line is there or beginning of next line is there,add data to main list and flush temporary one\n",
    "            if not is_sos and len(sentence_tokens)>0:\n",
    "                sentences.append(' '.join(sentence_tokens))\n",
    "                ner_labels.append(sentence_labels)\n",
    "                sentence_tokens,sentence_labels=[],[]\n",
    "    print('Done')\n",
    "    return sentences,ner_labels\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Done\n",
      "Reading data\n",
      "Done\n",
      "Reading data\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_sentences,train_labels=read_data(train_filepath)\n",
    "test_sentences,test_labels=read_data(test_filepath)\n",
    "valid_sentences,valid_labels=read_data(dev_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14987\n",
      "Valid size: 3466\n",
      "Test size: 3683\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_labels)}\")\n",
    "print(f\"Valid size: {len(valid_labels)}\")\n",
    "print(f\"Test size: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample data\n",
      "\n",
      "Sentence: -DOCSTART-\n",
      "Labels: ['O']\n",
      "\n",
      "\n",
      "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
      "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: LONDON 1996-08-30\n",
      "Labels: ['B-LOC', 'O']\n",
      "\n",
      "\n",
      "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
      "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some data\n",
    "print('\\nSample data\\n')\n",
    "for v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n",
    "    print(f\"Sentence: {v_sent}\")\n",
    "    print(f\"Labels: {v_labels}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One of the main characteristics of NER is class imbalance, all classes will not have equal number of samples. One observation is that there are more non name entities than name entities. Using chain method: convert all NER labels to a series object but these are list of list and inner list has the NER tags for all tokens in a sentence, so using chain() method to create a flat list. It chains several lists together to make a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label counts\n",
      "O         170524\n",
      "B-LOC       7140\n",
      "B-PER       6600\n",
      "B-ORG       6321\n",
      "I-PER       4528\n",
      "I-ORG       3704\n",
      "B-MISC      3438\n",
      "I-LOC       1157\n",
      "I-MISC      1155\n",
      "dtype: int64\n",
      "\n",
      "Validation data label counts\n",
      "O         42975\n",
      "B-PER      1842\n",
      "B-LOC      1837\n",
      "B-ORG      1341\n",
      "I-PER      1307\n",
      "B-MISC      922\n",
      "I-ORG       751\n",
      "I-MISC      346\n",
      "I-LOC       257\n",
      "dtype: int64\n",
      "\n",
      "Test data label counts\n",
      "O         38374\n",
      "B-ORG      1714\n",
      "B-LOC      1645\n",
      "B-PER      1617\n",
      "I-PER      1161\n",
      "I-ORG       881\n",
      "B-MISC      722\n",
      "I-LOC       259\n",
      "I-MISC      252\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking imbalamce of data\n",
    "from itertools import chain\n",
    "print(\"Training data label counts\")\n",
    "print(pd.Series(chain(*train_labels)).value_counts())\n",
    "print(\"\\nValidation data label counts\")\n",
    "print(pd.Series(chain(*valid_labels)).value_counts())\n",
    "print(\"\\nTest data label counts\")\n",
    "print(pd.Series(chain(*test_labels)).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O labels are several magnitudes higher than the volume of other labels. Also padding will be required so analyzing the sequence length of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14987.000000\n",
       "mean        13.649630\n",
       "std         11.700735\n",
       "min          1.000000\n",
       "5%           1.000000\n",
       "50%          9.000000\n",
       "95%         37.000000\n",
       "max        113.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyzing the sequence length\n",
    "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05,0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 95% of the sentences have 37 tokens or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing data: Labels need to be processed  \n",
    "Convert class labels to class ID  \n",
    "Pad sequence of labels to maximum length  \n",
    "Generate a mask to indicate padded labels to use this info to disregard paddded labels during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for mapping\n",
    "def get_label_id_map(train_labels):\n",
    "    # unique list of lebels\n",
    "    unique_train_labels=pd.Series(chain(*train_labels)).unique()\n",
    "    # create mapping using dictionary\n",
    "    labels_map=dict(zip(unique_train_labels,np.arange(unique_train_labels.shape[0])))\n",
    "    print(f\"label map:{labels_map}\")\n",
    "    return labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map:{'O': 0, 'B-ORG': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "labels_map=get_label_id_map(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_int_labels(labels, labels_map, max_seq_length, return_mask=True):\n",
    "\n",
    "    # Convert string labels to integers \n",
    "    int_labels = [[labels_map[x] for x in one_seq] for one_seq in labels]\n",
    "    \n",
    "    \n",
    "    # Pad sequences\n",
    "    if return_mask:\n",
    "        # If we return mask, we first pad with a special value (-1) and \n",
    "        # use that to create the mask and later replace -1 with 'O'\n",
    "        padded_labels = np.array(\n",
    "            tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                int_labels, maxlen=max_seq_length, padding='post', truncating='post', value=-1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # mask filter\n",
    "        mask_filter = (padded_labels != -1)\n",
    "        # replace -1 with 'O' s ID\n",
    "        padded_labels[~mask_filter] = labels_map['O']        \n",
    "        return padded_labels, mask_filter.astype('int')\n",
    "    \n",
    "    else:\n",
    "        padded_labels = np.array(ner_pad_sequence_func(int_labels, value=labels_map['O']))\n",
    "        return padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate processed labels and masks for train,test and valid data:\n",
    "max_seq_length=40\n",
    "padded_train_labels,train_mask=get_padded_int_labels(train_labels,labels_map,max_seq_length,return_mask=True)\n",
    "padded_test_labels,test_mask=get_padded_int_labels(test_labels,labels_map,max_seq_length,return_mask=True)\n",
    "padded_valid_labels,valid_mask=get_padded_int_labels(valid_labels,labels_map,max_seq_length,return_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Print some labels IDs\n",
    "print(padded_train_labels[:2])\n",
    "print(train_mask[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the hyperparameters:  \n",
    "The maximum length could be 40 as in previous code I saw that 95% sentences have 37 tokens or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=40\n",
    "embedding_size=64 # size of token embedding\n",
    "rnn_hidden_size=64 # number of hidden unites in RNN layer\n",
    "n_Classes=9 # number of o/p nodes\n",
    "batch_size=64 # number of samples in a batch\n",
    "epochs=4 # number of epochs to train\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the model: The model will have one embedding layer followed by a RNN layer and finally a Dense prediction layer, also integrating the tokenization in the model.This is done using TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With default arguments\n",
      "\n",
      "Data: \n",
      "[[8 5 7 2 3 4]\n",
      " [2 3 6 9 0 0]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'restaurent', 'yesterday', 'went', 'was', 'to', 'i', 'full']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "K.clear_session()\n",
    "# trying to fit on a small example\n",
    "toy_corpus=[\"I went to the restaurent yesterday\",\"The restaurent was full\"]\n",
    "toy_vectorization_layer=TextVectorization()\n",
    "# fit it on sample data\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "# generate output as\n",
    "toy_vectorized_output=toy_vectorization_layer(toy_corpus)\n",
    "# to see the vocabulary\n",
    "toy_vocabulary=toy_vectorization_layer.get_vocabulary()\n",
    "print(\"With default arguments\\n\")\n",
    "print(f\"Data: \\n{toy_vectorized_output}\")\n",
    "print(f\"Vocabulary: {toy_vocabulary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tokenization has done some preprocessing also like: converting to lower case. Also the size of vocabulary can be limited using the max_tokens argument, also preprocessing can be avoided by setting standardize to None, also the padding/truncation of sequences can be set with the output_sequence_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the rest of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "K.clear_session()\n",
    "# define an input layer with 1 column\n",
    "word_input=layers.Input(shape=(1,),dtype=tf.string)\n",
    "# define a function that takes corpus,maximum sequence length and vocabulary size and returns trained TextVectorization layer \n",
    "# and the vocabulary size\n",
    "def get_fitted_token_vectorization_layer(corpus,max_seq_length,vocabulary_size=None):\n",
    "    '''Fit a text vectorization layer on data'''\n",
    "    vectorization_layer=TextVectorization(max_tokens=vocabulary_size,standardize=None,output_sequence_length=max_seq_length)\n",
    "    # setting standardize to none because in NER keeping the case of letters is important as typically entity start with upper case\n",
    "    # fit on data\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    # get vocabulary size\n",
    "    n_vocab=len(vectorization_layer.get_vocabulary())\n",
    "    return vectorization_layer,n_vocab\n",
    "\n",
    "# text vectorized layer\n",
    "vectorized_layer,n_vocab=get_fitted_token_vectorization_layer(train_sentences,max_seq_length)\n",
    "# map each word to id : pass word input to vectorized layer to get vectorized output\n",
    "vectorized_output=vectorized_layer(word_input)\n",
    "# the output from vectorization layer is sent to embedding layer\n",
    "# look up embeddings for the ids\n",
    "embedding_layer=layers.Embedding(input_dim=n_vocab,output_dim=embedding_size,mask_zero=True)(vectorized_output)\n",
    "# define a simple RNN layer\n",
    "rnn_layer=layers.SimpleRNN(units=rnn_hidden_size,return_sequences=True)\n",
    "# arguments: units(int)- hidden output size. more is desired\n",
    "# return_sequences- whether to return sequence from all timesteps or to return last output, for NER every single\n",
    "# token needs to be labelled so its required to return sequences at all time steps\n",
    "rnn_out=rnn_layer(embedding_layer)\n",
    "# Defining dense layer\n",
    "dense_layer=layers.Dense(n_Classes,activation='softmax')\n",
    "# time-distributed output from the RNN will go to a Dense layer\n",
    "dense_out=dense_layer(rnn_out)\n",
    "# define the model\n",
    "model=tf.keras.Model(inputs=word_input,outputs=dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics and loss function: As earlier I saw the distribution of labels, it appears to be a class imbalance problem, this needs to be considered while training and evaluating the model so either a new metric can be used or sample weights can be used to penalize more frequent classes and boost importance of rare classes, so using first one.A modified version of accuracy known as macro accuarcy can be used where accuracies for each class are calculated and then averaged so class imbalance is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining macro accuracy:\n",
    "def macro_accuracy(y_true,y_pred):\n",
    "    # y_pred has predictions for all classes so getting the predicted class from it using argmax\n",
    "    # first convert y_true and y_pred to flattened vectors\n",
    "    y_true=tf.cast(tf.reshape(y_true,[-1]),'int32')\n",
    "    y_pred=tf.cast(tf.reshape(tf.argmax(y_pred,axis=-1),[-1]),'int32')\n",
    "    # sort y_true so that same labels are together\n",
    "    sorted_y_true=tf.sort(y_true)\n",
    "    sorted_inds=tf.argsort(y_true)\n",
    "    # use the tf.gather() function to order y_pred in the same order as y_true after sorting\n",
    "    sorted_y_pred=tf.gather(y_pred,sorted_inds)\n",
    "    # tf.gather() function takes a tensor and a set of indices and orders the passed tensor in the order of the indices.\n",
    "    # calculated sorted_corrected that is true if a particular element is equal in sorted_y_true and sorted_y_pred\n",
    "    sorted_corrected=tf.cast(tf.math.equal(sorted_y_true,sorted_y_pred),'int32')\n",
    "    # then use segment sum to calculate segmented sum of correctly predicted samples,\n",
    "    # samples belonging to each class considered a particular segment\n",
    "    # segment sum has 2 parameters: data and segment id; data can be float or int and a tensor,\n",
    "    # segment id is a tensor which can be int only, size should be equal to data first dimension,should be sorted\n",
    "    # and values inside it can be repeated\n",
    "    # ex= data=[5,1,7,2,3,4,1,3], segment_ids=[0,0,0,1,2,2,3,3] - if I start iterating 5,1,7 fall under same segment as \n",
    "    # the first 3 values in segment_ids tensor are same , so here 4 segments are there and in each segment perform sum \n",
    "    # of all element so here result is- [5+1+7,2,3+4,1+3] -> [13,2,7,4]\n",
    "    correct_for_each_label=tf.cast(tf.math.segment_sum(sorted_corrected,sorted_y_true),'float32')+1\n",
    "    # adding by 1 to avoid division by 0 error\n",
    "    # do same for vectors of 1 - get numbers of true samples for each class in data\n",
    "    all_for_each_label = tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true),sorted_y_true),'float32')+1\n",
    "    mean_accuracy=tf.reduce_mean(correct_for_each_label/all_for_each_label)\n",
    "    return mean_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization (TextVect (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 40, 64)            1512064   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 40, 64)            8256      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 40, 9)             585       \n",
      "=================================================================\n",
      "Total params: 1,520,905\n",
      "Trainable params: 1,520,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Wrap this function in meanmetricwrapper to produce tf.keras.metrics.Metric object which can be passed to compile function\n",
    "mean_accuracy_metric=tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy,name='macro_accuracy')\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=[mean_accuracy_metric])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model: before training the model, class imbalance needs to be tackled so writing a function to compute class weightsto generate sample weights further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(train_labels):\n",
    "    label_count=pd.Series(chain(*train_labels)).value_counts()\n",
    "    # to compute weights divide minimum frequency with other frequencies\n",
    "    label_count=label_count.min()/label_count\n",
    "    label_id_map=get_label_id_map(train_labels)\n",
    "    # output is converted into a dictionary that has class IDs as keys and class weights as values.\n",
    "    label_count.index=label_count.index.map(label_id_map)\n",
    "    return label_count.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weights_from_class_weights(labels,class_weights):\n",
    "    #perform a dictionary lookup element-wise on each label to generate a sample weight from class_weights\n",
    "    return np.vectorize(class_weights.get)(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map:{'O': 0, 'B-ORG': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "Class Weights:{0: 0.006773240130421524, 5: 0.16176470588235295, 3: 0.175, 1: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298185, 7: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Generate actual weights:\n",
    "train_class_weights=get_class_weights(train_labels)\n",
    "print(f\"Class Weights:{train_class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other has the lowest weight as it’s the most frequent and the class I-LOC has the highest as it’s the least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "235/235 [==============================] - 19s 55ms/step - loss: 0.0280 - macro_accuracy: 0.5645 - val_loss: 0.4295 - val_macro_accuracy: 0.7097\n",
      "Epoch 2/4\n",
      "235/235 [==============================] - 12s 52ms/step - loss: 0.0087 - macro_accuracy: 0.8870 - val_loss: 0.1640 - val_macro_accuracy: 0.7957\n",
      "Epoch 3/4\n",
      "235/235 [==============================] - 11s 47ms/step - loss: 0.0028 - macro_accuracy: 0.9604 - val_loss: 0.0928 - val_macro_accuracy: 0.8023\n",
      "Epoch 4/4\n",
      "235/235 [==============================] - 12s 49ms/step - loss: 0.0014 - macro_accuracy: 0.9772 - val_loss: 0.0806 - val_macro_accuracy: 0.7946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d82709220>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make train sequences an array\n",
    "train_sentences=np.array(train_sentences)\n",
    "# get sample weights\n",
    "train_sample_weights=get_sample_weights_from_class_weights(padded_train_labels,train_class_weights)\n",
    "# train the model\n",
    "model.fit(\n",
    "    train_sentences,padded_train_labels,sample_weight=train_sample_weights,batch_size=batch_size,epochs=epochs,\n",
    "    validation_data=(np.array(valid_sentences),padded_valid_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 1s 8ms/step - loss: 0.0901 - macro_accuracy: 0.7633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09011910110712051, 0.7633246779441833]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\t -DOCSTART-\n",
      "True:\t O\n",
      "Pred:\t O\n",
      "\n",
      "\n",
      "Sample:\t SOCCER\t-\tJAPAN\tGET\tLUCKY\tWIN\t,\tCHINA\tIN\tSURPRISE\tDEFEAT\t.\n",
      "True:\t O\tO\tB-LOC\tO\tO\tO\tO\tB-LOC\tO\tO\tO\tO\n",
      "Pred:\t O\tO\tB-MISC\tO\tO\tO\tO\tB-ORG\tO\tO\tO\tO\n",
      "\n",
      "\n",
      "Sample:\t Nadim\tLadki\n",
      "True:\t B-PER\tI-PER\n",
      "Pred:\t O\tO\n",
      "\n",
      "\n",
      "Sample:\t AL-AIN\t,\tUnited\tArab\tEmirates\t1996-12-06\n",
      "True:\t B-LOC\tO\tB-LOC\tI-LOC\tI-LOC\tO\n",
      "Pred:\t O\tO\tB-LOC\tI-LOC\tI-LOC\tO\n",
      "\n",
      "\n",
      "Sample:\t Japan\tbegan\tthe\tdefence\tof\ttheir\tAsian\tCup\ttitle\twith\ta\tlucky\t2-1\twin\tagainst\tSyria\tin\ta\tGroup\tC\tchampionship\tmatch\ton\tFriday\t.\n",
      "True:\t B-LOC\tO\tO\tO\tO\tO\tB-MISC\tI-MISC\tO\tO\tO\tO\tO\tO\tO\tB-LOC\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "Pred:\t B-LOC\tO\tO\tO\tO\tO\tB-MISC\tI-MISC\tI-MISC\tO\tO\tO\tO\tO\tO\tB-LOC\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyzing outputs:\n",
    "# using first 5 sentences\n",
    "n_samples=5\n",
    "visual_test_sentences=test_sentences[:n_samples]\n",
    "visual_test_labels=padded_test_labels[:n_samples]\n",
    "visual_test_predictions=model.predict(np.array(visual_test_sentences))\n",
    "visual_test_pred_labels=np.argmax(visual_test_predictions,axis=-1)\n",
    "# create a reversed labels_map that has a mapping from label ID to label string:\n",
    "rev_labels_map=dict(zip(labels_map.values(),labels_map.keys()))\n",
    "for i, (sentence, sent_labels, sent_preds) in enumerate(zip(visual_test_sentences, visual_test_labels, visual_test_pred_labels)):    \n",
    "    n_tokens = len(sentence.split())\n",
    "    print(\"Sample:\\t\",\"\\t\".join(sentence.split()))\n",
    "    print(\"True:\\t\",\"\\t\".join([rev_labels_map[i] for i in sent_labels[:n_tokens]]))\n",
    "    print(\"Pred:\\t\",\"\\t\".join([rev_labels_map[i] for i in sent_preds[:n_tokens]]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7773864c198c8559e499b8a6d42753464881661d1a635729c4702e1dcc7c46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
